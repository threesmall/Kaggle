{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Introduction: Taxi Fare Prediction###\n\n\nWelcome to the Taxi Fare Kaggle challenge. In this contest, the aim is to predict the fare of a taxi ride given the starting time, the starting and ending latitude / longitude, and the number of passengers. \n\nIn this kernel I am going to demonstrate the basic steps to tackle with a real life machine learning challenge. ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We would keep the first 2m records for faster processing","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/new-york-city-taxi-fare-prediction/train.csv\", nrows = 2000000)\ntest_data = pd.read_csv(\"/kaggle/input/new-york-city-taxi-fare-prediction/test.csv\", nrows = 20000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape)\nprint(test_data.shape)\n\ntrain_data[\"dataset\"] = \"train\"\ntest_data[\"dataset\"] = \"test\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.concat((train_data, test_data))\nX.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X = train_data.drop((\"fare_amount\"), axis = 1)\n# y = train_data[[\"fare_amount\"]]\n# X = train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train columns with null values:\\n', X.isnull().sum())\nprint(\"-\"*10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(7, 7))\n_ = sns.distplot(X[[\"fare_amount\"]], fit=norm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exclude Outlier\n\nThe starting fare is $2.5. So we exclude fares with extreme values. \n\nsource : https://www1.nyc.gov/site/tlc/passengers/taxi-fare.page#:~:text=%242.50%20initial%20charge.,Dutchess%2C%20Orange%20or%20Putnam%20Counties.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X[X['fare_amount'].between(left = 2.5, right = 100)& (X[\"dataset\"]== \"train\") | (X[\"dataset\"]==\"test\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 10))\nk = [\"pickup_longitude\", \"pickup_latitude\",\"dropoff_longitude\",\"dropoff_longitude\"]\nfor i, field in enumerate(k, 1):\n    plt.subplot(2,2,i)\n    _ = sns.kdeplot(X[field])\n    plt.xlabel([field])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']:\n    print(f'{col.capitalize():20}: 2.5% = {round(np.percentile(X[col], 2.5), 2):5} \\t 97.5% = {round(np.percentile(X[col], 97.5), 2)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exclude the outlier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# valid location of New York Manhattan -73.844311\t40.721319\n# X = X[(X[\"pickup_longitude\"] != 0) & (X[\"dropoff_latitude\"] != 0)& (X[\"pickup_latitude\"] != 0)& (X[\"dropoff_longitude\"] != 0)]\nX = X[(X[\"pickup_longitude\"] < -70) & (X[\"pickup_longitude\"] > -76) & (X[\"pickup_latitude\"] < 43) & (X[\"pickup_latitude\"] > 37) &\n     (X[\"dropoff_longitude\"] < -70) & (X[\"dropoff_longitude\"] > -76) & (X[\"dropoff_latitude\"] < 43) & (X[\"dropoff_latitude\"] > 37)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize = (20, 8), sharex=True, sharey=True)\naxes = axes.flatten()\n\n# Plot Longitude (x) and Latitude (y)\nsns.regplot('pickup_longitude', 'pickup_latitude', fit_reg = False, \n            data  = X.sample(10000, random_state = 100), ax = axes[0]);\nsns.regplot('dropoff_longitude', 'dropoff_latitude', fit_reg = False, \n            data  = X.sample(10000, random_state = 100), ax = axes[1]);\naxes[0].set_title('Pickup Locations')\naxes[1].set_title('Dropoff Locations');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot the folium map and we can see some coordinates falling in the sea. They should be wrong inputs but let's disregard them at this moment.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# sas install folium\nimport folium\nfrom folium.plugins import MarkerCluster\nfrom folium import plugins\nfrom folium.plugins import HeatMap\n\ndf_txn_display_pickup = X[:50000]\n\nm=folium.Map([ 40.72,-74.00],zoom_start=10)\nHeatMap(df_txn_display_pickup[['dropoff_latitude','dropoff_longitude']].dropna(),radius=8,gradient={0.2:'blue',0.4:'purple',0.6:'orange',1.0:'red'}).add_to(m)\nm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n=folium.Map([ 40.72,-74.00],zoom_start=10)\nHeatMap(df_txn_display_pickup[['pickup_latitude','pickup_longitude']].dropna(),radius=8,gradient={0.2:'blue',0.4:'purple',0.6:'orange',1.0:'red'}).add_to(n)\nn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Binning the fare \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.groupby(\"dataset\")[\"dataset\"].agg(\"count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bin the fare and convert to string\nX['fare-bin'] = pd.cut(X['fare_amount'], bins = list(range(0, 50, 5))).astype(str)\n\n# Uppermost bin\nX.loc[X['fare-bin'] == 'nan', 'fare-bin'] = '[45+]'\n\n# Adjust bin so the sorting is correct\nX.loc[X['fare-bin'] == '(5.0, 10.0]', 'fare-bin'] = '(05, 10]'\n\n# Bar plot of value counts\nX['fare-bin'].value_counts().sort_index().plot.bar(color = 'b', edgecolor = 'k');\nplt.title('Fare Binned');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ecdf(x):\n    \"\"\"Empirical cumulative distribution function of a variable\"\"\"\n    # Sort in ascending order\n    x = np.sort(x)\n    n = len(x)\n    \n    # Go from 1/n to 1\n    y = np.arange(1, n + 1, 1) / n\n    \n    return x, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xs, ys = ecdf(X['fare_amount'])\nplt.figure(figsize = (8, 6))\nplt.plot(xs, ys, '.')\nplt.ylabel('Percentile')\nplt.title('ECDF of Fare Amount')\nplt.xlabel('Fare Amount ($)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X['passenger_count'].value_counts().plot.bar(color = 'b', edgecolor = 'k');\nplt.title('Passenger Counts'); plt.xlabel('Number of Passengers'); plt.ylabel('Count');\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X[X['passenger_count'].between(left = 1, right = 6)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a color mapping based on fare bins\npalette = sns.color_palette('Paired', 10)\ncolor_mapping = {fare_bin: palette[i] for i, fare_bin in enumerate(X['fare-bin'].unique())}\ncolor_mapping\n\nX['color'] = X['fare-bin'].map(color_mapping)\nplot_data = X.sample(1_000_000, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BB_zoom = (-74.1, -73.7, 40.6, 40.85)\nnyc_map_zoom = plt.imread('https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/images/nyc_-74.1_-73.7_40.6_40.85.PNG?raw=true')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig, axs = plt.subplots(1, 1, figsize=(20, 18))\n\n# Plot the pickups\nfor b, df in plot_data.groupby('fare-bin'):\n    # Set the zorder to 1 to plot on top of map\n    axs.scatter(df.pickup_longitude, df.pickup_latitude, zorder=1, alpha=0.2, c=df.color, s=30, label = f'{b}')\n    axs.set_title('Pickup locations', size = 22)\n    axs.axis('off')\n\n# Legend\nleg = axs.legend(fontsize = 14, markerscale = 3)\n\n# Adjust alpha of legend markers\nfor lh in leg.legendHandles: \n    lh.set_alpha(1)\n\nleg.set_title('Fare Bin', prop = {'size': 28})\n\n# Show map in background (zorder = 0)\naxs.imshow(nyc_map_zoom, zorder=0, extent=BB_zoom);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Absolute difference in latitude and longitude\nX['abs_lat_diff'] = (X['dropoff_latitude'] - X['pickup_latitude']).abs()\nX['abs_lon_diff'] = (X['dropoff_longitude'] - X['pickup_longitude']).abs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot('abs_lat_diff', 'abs_lon_diff', hue = 'fare-bin', size = 8, palette=palette,\n           fit_reg = False, data = X.sample(10000, random_state=100));\nplt.title('Absolute latitude difference vs Absolute longitude difference');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot('abs_lat_diff', 'abs_lon_diff', hue = 'fare-bin', size = 8, palette=palette,\n           fit_reg = False, data = X.sample(10000, random_state=100));\nplt.title('Absolute latitude difference vs Absolute longitude difference');\n\nplt.xlim((-0.01, .25)); plt.ylim((-0.01, .25))\nplt.title('Absolute latitude difference vs Absolute longitude difference');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_diff = X[(X['abs_lat_diff'] == 0) & (X['abs_lon_diff'] == 0)]\nno_diff.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is 20788 records with no change in distance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime as dt\nimport time\n\n# X[\"fulldatetime\"] = X[\"pickup_datetime\"]\n# X[\"fulldatetime\"] = [a.strip(\" UTC\") for a in X[\"fulldatetime\"]]\n# X[\"date\"] = [dt.dt.strptime(b, '%Y-%m-%d %H:%M:%S').date() for b in X[\"fulldatetime\"]]\n# X[\"time\"] = [datetime.datetime.strptime(b, '%Y-%m-%d %H:%M:%S').time() for b in X[\"fulldatetime\"]]\n\ndef add_datetime_info(dataset):\n    #Convert to datetime format\n    dataset['pickup_datetime'] = pd.to_datetime(dataset['pickup_datetime'],format=\"%Y-%m-%d %H:%M:%S UTC\")\n    \n    dataset['hour'] = dataset.pickup_datetime.dt.hour\n    dataset['day'] = dataset.pickup_datetime.dt.day\n    dataset['month'] = dataset.pickup_datetime.dt.month\n    dataset['weekday'] = dataset.pickup_datetime.dt.weekday\n    dataset['year'] = dataset.pickup_datetime.dt.year\n    \n    return dataset\n\nadd_datetime_info(X)\n\n# Convert date to ordinal date \n# example :\n# >>> date2\n# datetime.datetime(2012, 2, 2, 0, 0)\n# >>> tstamp = time.mktime(date2.timetuple())\n# >>> tstamp\n# 1328121000.0\n\nX[\"fulldatetimetuple\"] = [time.mktime(a.timetuple()) for a in X[\"pickup_datetime\"]]\n\n\n# Euclidean Distance \nX[\"straightdist\"] = ((X[\"pickup_longitude\"] - X[\"dropoff_longitude\"])**2 + (X[\"pickup_latitude\"] - X[\"dropoff_latitude\"])**2)**0.5\n\n# Manhattan Distance\nX[\"manhattan\"] = X['abs_lat_diff'] + X['abs_lon_diff'] \n\n\n# Haversine Distance \nfrom haversine import haversine, Unit\n\nX['pickup'] = list(zip(X.pickup_latitude, X.pickup_longitude))\nX['dropoff'] = list(zip(X.dropoff_latitude, X.dropoff_longitude))\n\ndef calculate_haversine(row):\n    dis = haversine(row['pickup'], row['dropoff'])\n    return dis\n\nX[\"haverdist\"] = X.apply(calculate_haversine, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"  \n # Calculate distribution by each fare bin\nplt.figure(figsize = (12, 6))\nfor f, grouped in X.groupby('fare-bin'):\n    sns.kdeplot(grouped['manhattan'], label = f'{f}', color = list(grouped['color'])[0]);\n\nplt.xlabel('degrees'); plt.ylabel('density')\nplt.title('Manhattan Distance by Fare Amount');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"  \n # Calculate distribution by each fare bin\nplt.figure(figsize = (12, 6))\nfor f, grouped in X.groupby('fare-bin'):\n    sns.kdeplot(grouped['haverdist'], label = f'{f}', color = list(grouped['color'])[0]);\n\nplt.xlabel('degrees'); plt.ylabel('density')\nplt.title('Haverdist Distance by Fare Amount');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate distribution by each fare bin\nplt.figure(figsize = (12, 6))\nfor f, grouped in X.groupby('fare-bin'):\n    sns.kdeplot(grouped['straightdist'], label = f'{f}', color = list(grouped['color'])[0]);\n\nplt.xlabel('degrees'); plt.ylabel('density')\nplt.title('Euclidean Distance by Fare Amount');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.groupby('fare-bin')['straightdist'].agg(['mean', 'count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.groupby('fare-bin')['straightdist'].mean().plot.bar(color = 'b');\nplt.title('Average Euclidean Distance by Fare Bin');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\n\nfor p, grouped in X.groupby('passenger_count'):\n    sns.kdeplot(grouped['fare_amount'], label = f'{p} passengers', color = list(grouped['color'])[0]);\n    \nplt.xlabel('Fare Amount'); plt.ylabel('Density')\nplt.title('Distribution of Fare Amount by Number of Passengers');\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.groupby('passenger_count')['fare_amount'].agg(['mean', 'count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.groupby('passenger_count')['fare_amount'].mean().plot.bar(color = 'b');\nplt.title('Average Fare by Passenger Count');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# f, ax = plt.subplots(figsize=(7, 7))\n# _ = sns.pairplot(X.sample(1000, random_state = 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs = X.corr()\ncorrs['fare_amount'].plot.bar(color = 'b');\nplt.title('Correlation with Fare Amount');\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(7, 7))\nsns.lineplot(x=\"year\", y=\"fare_amount\",data=X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Year : fare amount is higher generally in post 2013 compared to pre 2013 \n\n\n#### Weekday / day / month : no trend observed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 10))\nk = [\"year\", \"weekday\",\"month\",\"day\"]\nfor i, field in enumerate(k, 1):\n    plt.subplot(2,2,i)\n    _ = sns.boxplot(y = X[\"fare_amount\"], x = X[field], palette=\"Blues\")\n    plt.xlabel([field])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 10))\nk = [\"straightdist\", \"hour\"]\nfor i, field in enumerate(k, 1):\n    plt.subplot(2,1,i)\n    _ = sns.scatterplot(y = X[\"fare_amount\"], x = X[field], palette=\"Blues\")\n    plt.xlabel([field])\n    \n# sns.scatterplot(x=\"straightdist\", y=\"fare_amount\",data=X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# f, ax = plt.subplots(figsize=(10, 10))\ne = sns.FacetGrid(X, col =  \"dataset\")\ne.map(sns.scatterplot, \"haverdist\", \"fare_amount\") \n\n#no fare amount in test dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = sns.FacetGrid(X, col =  \"dataset\")\nf.map(sns.distplot, \"haverdist\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only 412 records with haverdist > 30km out of 2m records\nX[X[\"haverdist\"]>100]\n\nX = X[X[\"haverdist\"]<100]\n\n# delete negative fare for training data\nX = X[ ((X[\"fare_amount\"]>0) & (X[\"dataset\"]== \"train\")) | (X[\"dataset\"]==\"test\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X[(X[\"haverdist\"]<0.2) & (X[\"fare_amount\"]>10)] \n\n# X = X.drop(X[(X[\"haverdist\"]<0.2) & (X[\"fare_amount\"]>10)].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\n\nk = {\"dropoff\" : [\"dropoff_longitude\",  \"dropoff_latitude\"],\n     \"pickup\" : [\"pickup_longitude\",  \"pickup_latitude\"]}\nfor i, field in enumerate(k, 1):\n    plt.subplot(2,1,i)\n    _ = sns.scatterplot(y = X[k[field][1]], x = X[k[field][0]], palette=\"Blues\")\n    plt.xlabel(k[field][0])\n    plt.ylabel(k[field][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sas install folium\nimport folium\nfrom folium.plugins import MarkerCluster\nfrom folium import plugins\nfrom folium.plugins import HeatMap\n\ndf_txn_display_pickup = X[:50000]\n\nm=folium.Map([ 40.72,-74.00],zoom_start=10)\nHeatMap(df_txn_display_pickup[['dropoff_latitude','dropoff_longitude']].dropna(),radius=8,gradient={0.2:'blue',0.4:'purple',0.6:'orange',1.0:'red'}).add_to(m)\nm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n=folium.Map([ 40.72,-74.00],zoom_start=10)\nHeatMap(df_txn_display_pickup[['pickup_latitude','pickup_longitude']].dropna(),radius=8,gradient={0.2:'blue',0.4:'purple',0.6:'orange',1.0:'red'}).add_to(n)\nn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[X[\"straightdist\"]==None]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop(X[X[\"straightdist\"].isna()].index)  #drop straightdist = NA\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[X.isna().any(axis=1)]  #display all NA ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#histogram and normal probability plot\\\nfrom scipy.stats import norm\nimport scipy.stats as stats\n\nsns.distplot(X['fare_amount'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(X['fare_amount'],  plot=plt)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[\"logfare\"] = np.log1p(X['fare_amount'])\n\nsns.distplot(X[\"logfare\"] , fit=norm);\nfig = plt.figure()\nres = stats.probplot(X['logfare'],  plot=plt)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.special import boxcox1p\nX.skew(axis = 0, skipna = True) \nlam = 0.15\n\nX[\"skewhaverdist\"] = boxcox1p(X[\"haverdist\"], lam)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndef extract_dateinfo(df, date_col, drop=True, time=False, \n                     start_ref = pd.datetime(1900, 1, 1),\n                     extra_attr = False):\n    \"\"\"\n    Extract Date (and time) Information from a DataFrame\n    Adapted from: https://github.com/fastai/fastai/blob/master/fastai/structured.py\n    \"\"\"\n    df = df.copy()\n    \n    # Extract the field\n    fld = df[date_col]\n    \n    # Check the time\n    fld_dtype = fld.dtype\n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        fld_dtype = np.datetime64\n\n    # Convert to datetime if not already\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[date_col] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n    \n\n    # Prefix for new columns\n    pre = re.sub('[Dd]ate', '', date_col)\n    pre = re.sub('[Tt]ime', '', pre)\n    \n    # Basic attributes\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Days_in_month', 'is_leap_year']\n    \n    # Additional attributes\n    if extra_attr:\n        attr = attr + ['Is_month_end', 'Is_month_start', 'Is_quarter_end', \n                       'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    \n    # If time is specified, extract time information\n    if time: \n        attr = attr + ['Hour', 'Minute', 'Second']\n        \n    # Iterate through each attribute\n    for n in attr: \n        df[pre + n] = getattr(fld.dt, n.lower())\n        \n    # Calculate days in year\n    df[pre + 'Days_in_year'] = df[pre + 'is_leap_year'] + 365\n        \n    if time:\n        # Add fractional time of day (0 - 1) units of day\n        df[pre + 'frac_day'] = ((df[pre + 'Hour']) + (df[pre + 'Minute'] / 60) + (df[pre + 'Second'] / 60 / 60)) / 24\n        \n        # Add fractional time of week (0 - 1) units of week\n        df[pre + 'frac_week'] = (df[pre + 'Dayofweek'] + df[pre + 'frac_day']) / 7\n    \n        # Add fractional time of month (0 - 1) units of month\n        df[pre + 'frac_month'] = (df[pre + 'Day'] + (df[pre + 'frac_day'])) / (df[pre + 'Days_in_month'] +  1)\n        \n        # Add fractional time of year (0 - 1) units of year\n        df[pre + 'frac_year'] = (df[pre + 'Dayofyear'] + df[pre + 'frac_day']) / (df[pre + 'Days_in_year'] + 1)\n        \n    # Add seconds since start of reference\n    df[pre + 'Elapsed'] = (fld - start_ref).dt.total_seconds()\n    \n    if drop: \n        df = df.drop(date_col, axis=1)\n        \n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = extract_dateinfo(X, 'pickup_datetime', drop = False, \n                         time = True, start_ref = X['pickup_datetime'].min())\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 12))\ncorrs[\"fare_amount\"].sort_values(ascending=False).plot.bar(color = \"red\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs = X.corr()\n\nplt.figure(figsize = (12, 12))\ncorrs[\"fare_amount\"].sort_values(ascending=False).plot.bar(color = \"red\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[X.isna().any(axis=1)]  #display all NA ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_y = X[X[\"dataset\"] == \"test\"]\n# test_y = test_y[[\"fare_amount\"]].values\n\n# filter_test = X[\"dataset\"] == \"test\"\n# filter_train = X[\"dataset\"] == \"train\"\n\n# train_y  = X[filter_train][[\"fare_amount\"]].values\n# train_X  = X[filter_train][[\"haverdist\"]].values\n# test_X  = X[filter_test][[\"haverdist\"]].values\n\n# features = ['year', 'hour', 'haverdist', 'passenger_count']\n# features = ['year', 'hour', 'skewhaverdist', 'passenger_count']\n# features_y = ['logfare']\n# features = [ 'haverdist']\n\n# train_y  = X[filter_train][features_y].values\n# train_X  = X[filter_train][features].values\n# test_X  = X[filter_test][features].values\n\n# train_X = X[filter_train].drop(features_todrop, axis = 1).values\n# test_X = X[filter_test].drop(features_todrop, axis = 1).values\n# train_X = X[filter_train].drop([\"key\",\"fare_amount\",\"pickup_datetime\",\"dataset\",\"pickup\",\"dropoff\"], axis = 1).values\n# test_X = X[filter_test].drop([\"key\",\"fare_amount\",\"pickup_datetime\",\"dataset\",\"pickup\",\"dropoff\"], axis = 1).values\n# filtering data \n# data.where(filter, inplace = True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_tr, X_va, y_tr, y_va = train_test_split(X[X[\"dataset\"]==\"train\"], np.array(X[X[\"dataset\"]==\"train\"]['fare_amount']), test_size=0.25, random_state  = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore', category = RuntimeWarning)\n\ndef metrics(train_pred, valid_pred, y_train, y_valid):\n    \"\"\"Calculate metrics:\n       Root mean squared error and mean absolute percentage error\"\"\"\n    \n    # Root mean squared error\n    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n    valid_rmse = np.sqrt(mean_squared_error(y_valid, valid_pred))\n    \n    # Calculate absolute percentage error\n    train_ape = abs((y_train - train_pred) / y_train)\n    valid_ape = abs((y_valid - valid_pred) / y_valid)\n    \n    # Account for y values of 0\n    train_ape[train_ape == np.inf] = 0\n    train_ape[train_ape == -np.inf] = 0\n    valid_ape[valid_ape == np.inf] = 0\n    valid_ape[valid_ape == -np.inf] = 0\n    \n    train_mape = 100 * np.mean(train_ape)\n    valid_mape = 100 * np.mean(valid_ape)\n    \n    return train_rmse, valid_rmse, train_mape, valid_mape\n\ndef evaluate(model, features, X_train, X_valid, y_train, y_valid):\n    \"\"\"Mean absolute percentage error\"\"\"\n    \n    # Make predictions\n    train_pred = model.predict(X_train[features])\n    valid_pred = model.predict(X_valid[features])\n    \n    # Get metrics\n    train_rmse, valid_rmse, train_mape, valid_mape = metrics(train_pred, valid_pred,\n                                                             y_train, y_valid)\n    \n    print(f'Training:   rmse = {round(train_rmse, 2)} \\t mape = {round(train_mape, 2)}')\n    print(f'Validation: rmse = {round(valid_rmse, 2)} \\t mape = {round(valid_mape, 2)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_tr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 12))\ncorrs[\"fare_amount\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n\n# y_tr is ['fare_amount']\n# features = ['year', 'hour', 'skewhaverdist', 'passenger_count']\nfeatures = ['abs_lon_diff', 'abs_lat_diff','haverdist', 'passenger_count']\nmodel = LinearRegression()\nmodel.fit(X_tr[features], y_tr)\npy = model.predict(X_tr[features])\npy_val = model.predict(X_va[features])\nrmse = np.sqrt(mean_squared_error(y_tr, py))\nrmse2 = np.sqrt(mean_squared_error(y_va, py_val))\nprint(\"rmse for training:\", rmse)\nprint(\"rmse for validation:\", rmse2)\n\n# change y_tr as y_tr2 = ['logfare']\ny_tr2  = np.array(X_tr['logfare'])\ny_va2  = np.array(X_va['logfare'])\nmodel.fit(X_tr[features], y_tr2)\npy2 = model.predict(X_tr[features])\npy2_val = model.predict(X_va[features])\nrmse__2 = np.sqrt(mean_squared_error(y_tr2, py2))\nrmse__2_val = np.sqrt(mean_squared_error(y_va2, py2_val))\nprint(\"rmse for training:\", rmse__2)\nprint(\"rmse for validation:\", rmse__2_val)\n\n\n# def cv_rmse(model, X, y):\n#     rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv = 5).mean())\n#     return print(rmse)\n# cv_rmse(LinearRegression(), train_X, train_y)\n# model.score(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use Bill Method \n# from sklearn.model_selection import train_test_split\n# X_tr, X_va, y_tr, y_va = train_test_split(X, np.array(X['fare_amount']), test_size=0.25)\n\nlr = LinearRegression()\nfeatures = ['abs_lon_diff', 'abs_lat_diff','haverdist', 'passenger_count']\n\n\nlr.fit(X_tr[features], y_tr)\n\nprint('Intercept', round(lr.intercept_, 4))\nprint('abs_lat_diff coef: ', round(lr.coef_[0], 4), \n      '\\tabs_lon_diff coef:', round(lr.coef_[1], 4),\n      '\\thaverdist coef:', round(lr.coef_[2], 4),\n      '\\tpassenger_count coef:', round(lr.coef_[3], 4)\n     )\n\nevaluate(lr, features, X_tr, X_va, y_tr, y_va)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['abs_lat_diff', 'abs_lon_diff', 'haverdist']\n\n\nlr.fit(X_tr[features], y_tr)\n\nprint('Intercept', round(lr.intercept_, 4))\nprint('abs_lat_diff coef: ', round(lr.coef_[0], 4), \n      '\\tabs_lon_diff coef:', round(lr.coef_[1], 4),\n      '\\thaverdist coef:', round(lr.coef_[2], 4)\n     )\n\nevaluate(lr, features, X_tr, X_va, y_tr, y_va)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Use Random Forest : RMSE is better @3.35, but we found overfitting to validation dataset. We use pruning to mitigate ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# Create the random forest\nrandom_forest = RandomForestRegressor(n_estimators = 20, max_depth = 20, \n                                      max_features = None, oob_score = True, \n                                      bootstrap = True, verbose = 1, n_jobs = -1)\n\n# Train on data\nrandom_forest.fit(X_tr[['haverdist', 'abs_lat_diff', 'abs_lon_diff', 'passenger_count']], y_tr)\nevaluate(random_forest, ['haverdist', 'abs_lat_diff', 'abs_lon_diff', 'passenger_count'],\n         X_tr, X_va, y_tr, y_va)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfeature_importances = pd.DataFrame({'feature': ['haverdist', 'abs_lat_diff', 'abs_lon_diff', 'passenger_count'],\n                                        'importance': random_forest.feature_importances_}).\\\n                           sort_values('importance', ascending = False).set_index('feature')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances.plot.bar(color = 'b', edgecolor = 'k', linewidth = 2);\nplt.title('Feature Importances');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tr.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest.fit(X_tr[['haverdist', 'abs_lat_diff', 'abs_lon_diff', 'passenger_count','pickup_Elapsed','manhattan','pickup_longitude','pickup_latitude', 'dropoff_longitude','dropoff_latitude']], y_tr)\nevaluate(random_forest, ['haverdist', 'abs_lat_diff', 'abs_lon_diff', 'passenger_count','pickup_Elapsed','manhattan','pickup_longitude','pickup_latitude', 'dropoff_longitude','dropoff_latitude'],\n         X_tr, X_va, y_tr, y_va)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pruning : cut max_depth from 20 to 10; however, result is not satisfactory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# Create the random forest\nrandom_forest = RandomForestRegressor(n_estimators = 20, max_depth = 10, \n                                      max_features = None, oob_score = True, \n                                      bootstrap = True, verbose = 1, n_jobs = -1)\n\n# Train on data\nrandom_forest.fit(X_tr[['haverdist', 'abs_lat_diff', 'abs_lon_diff', 'passenger_count','pickup_Elapsed','manhattan','pickup_longitude','pickup_latitude', 'dropoff_longitude','dropoff_latitude']], y_tr)\nevaluate(random_forest, ['haverdist', 'abs_lat_diff', 'abs_lon_diff', 'passenger_count','pickup_Elapsed','manhattan','pickup_longitude','pickup_latitude', 'dropoff_longitude','dropoff_latitude'],\n         X_tr, X_va, y_tr, y_va)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression + Standard Scalar","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# define some handy analysis support function\nfrom sklearn.metrics import mean_squared_error, explained_variance_score\n\ndef plot_prediction_analysis(y, y_pred, figsize=(10,4), title=''):\n    fig, axs = plt.subplots(1, 2, figsize=figsize)\n    axs[0].scatter(y, y_pred)\n    mn = min(np.min(y), np.min(y_pred))\n    mx = max(np.max(y), np.max(y_pred))\n    axs[0].plot([mn, mx], [mn, mx], c='red')\n    axs[0].set_xlabel('$y$')\n    axs[0].set_ylabel('$\\hat{y}$')\n    rmse = np.sqrt(mean_squared_error(y, y_pred))\n    evs = explained_variance_score(y, y_pred)\n    axs[0].set_title('rmse = {:.2f}, evs = {:.2f}'.format(rmse, evs))\n    \n    axs[1].hist(y-y_pred, bins=50)\n    avg = np.mean(y-y_pred)\n    std = np.std(y-y_pred)\n    axs[1].set_xlabel('$y - \\hat{y}$')\n    axs[1].set_title('Histrogram prediction error, $\\mu$ = {:.2f}, $\\sigma$ = {:.2f}'.format(avg, std))\n    \n    if title!='':\n        fig.suptitle(title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\nmodel_lin = Pipeline((\n        (\"standard_scaler\", StandardScaler()),\n        (\"lin_reg\", LinearRegression()),\n    ))\n\n\nfeatures = ['haverdist', 'abs_lat_diff', 'abs_lon_diff', 'passenger_count']\nmodel_lin.fit(X_tr[features], y_tr)\n\n\ny_train_pred = model_lin.predict(X_tr[features])\nplot_prediction_analysis(y_tr, y_train_pred, title='Linear Model - Trainingset')\n\ny_test_pred = model_lin.predict(X_va[features])\nplot_prediction_analysis(y_va, y_test_pred, title='Linear Model - Testset')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_rmse_analysis(model, X, y, N=400, test_size=0.25, figsize=(10,4), title=''):\n    rmse_train, rmse_test = [], []\n    for i in range(N):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n\n        model.fit(X_train, y_train)\n        y_train_pred = model.predict(X_train)\n        y_test_pred = model.predict(X_test)\n\n        rmse_train.append(np.sqrt(mean_squared_error(y_train, y_train_pred)))\n        rmse_test.append(np.sqrt(mean_squared_error(y_test, y_test_pred)))\n\n    g = sns.jointplot(np.array(rmse_train), np.array(rmse_test), kind='scatter', stat_func=None, size=5)\n    g.set_axis_labels(\"RMSE training ($\\mu$={:.2f})\".format(np.mean(rmse_train)), \n                      \"RMSE test ($\\mu$={:.2f})\".format(np.mean(rmse_test)))\n    plt.subplots_adjust(top=0.9)\n    g.fig.suptitle('{} (N={}, test_size={:0.2f})'.format(title, N, test_size))\n\nplot_rmse_analysis(model_lin, X_tr[features], y_tr, title='Linear model')  \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tr.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding time features ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"time_features = ['pickup_Year', 'pickup_Month',\n       'pickup_Week', 'pickup_Day', 'pickup_Dayofweek', 'pickup_Dayofyear',\n       'pickup_Days_in_month', 'pickup_is_leap_year', 'pickup_Hour',\n       'pickup_Minute', 'pickup_Second', 'pickup_Days_in_year',\n       'pickup_frac_day', 'pickup_frac_week', 'pickup_frac_month',\n       'pickup_frac_year', 'pickup_Elapsed']\n\nfeatures2 = ['abs_lat_diff', 'abs_lon_diff', 'haverdist', 'passenger_count',\n            'pickup_latitude', 'pickup_longitude', \n            'dropoff_latitude', 'dropoff_longitude','manhattan'] + time_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# Create the random forest\nrandom_forest = RandomForestRegressor(n_estimators = 20, max_depth = 20, \n                                      max_features = None, oob_score = True, \n                                      bootstrap = True, verbose = 1, n_jobs = -1)\n\n# Train on data\nrandom_forest.fit(X_tr[features2], y_tr)\nevaluate(random_forest, features2,\n         X_tr, X_va, y_tr, y_va)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = X_tr[features2].columns.tolist()\ncol2 = np.array(col)\ncol2.shape\n\nimport pandas as pd\nfeature_importances = pd.DataFrame({'feature': col2,\n                                        'importance': random_forest.feature_importances_}).\\\n                           sort_values('importance', ascending = False).set_index('feature')\n\nfeature_importances.plot.bar(color = 'b', edgecolor = 'k', linewidth = 2);\nplt.title('Feature Importances');\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fineturning the parameters to mitigate overfitting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\n# Hyperparameter grid\nparam_grid = {\n    'n_estimators': np.linspace(10, 100).astype(int),\n    'max_depth': [None] + list(np.linspace(5, 30).astype(int)),\n    'max_features': ['auto', 'sqrt', None] + list(np.arange(0.5, 1, 0.1)),\n    'max_leaf_nodes': [None] + list(np.linspace(10, 50, 500).astype(int)),\n    'min_samples_split': [2, 5, 10],\n    'bootstrap': [True, False]\n}\n\n# Estimator for use in random search\nestimator = RandomForestRegressor(random_state = 100)\n\n# Create the random search model, change \"interation\"\nrs = RandomizedSearchCV(estimator, param_grid, n_jobs = -1, \n                        scoring = 'neg_mean_absolute_error', cv = 3, \n                        n_iter = 5, verbose = 1, random_state=100)\n\n\n# Train on data\nrs.fit(X_tr[features2], y_tr)\nbestmodel = rs.best_estimator_\nbestmodelscore = rs.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bestmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_bestrf = bestmodel.predict(X_tr[features2])\nrmse = np.sqrt(mean_squared_error(y_train_bestrf, y_tr))\nprint(\"train_rmse = \", rmse)\n\ny_valid_bestrf  = bestmodel.predict(X_va[features2])\nrmse = np.sqrt(mean_squared_error(y_valid_bestrf , y_va))\nprint(\"valid_rmse = \", rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting ###","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgbm\n\n# features = ['haverdist', 'abs_lat_diff', 'abs_lon_diff', 'passenger_count','pickup_Elapsed','manhattan','pickup_longitude','pickup_latitude', 'dropoff_longitude','dropoff_latitude']\n\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'nthread': -1,\n        'verbose': 0,\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'subsample_freq': 1,\n        'colsample_bytree': 0.6,\n        'reg_aplha': 1,\n        'reg_lambda': 0.001,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1     \n    }\ntrain_set = lgbm.Dataset(X_tr[features2], y_tr)\nvalid_set = lgbm.Dataset(X_va[features2], y_va)\n\ngb = lgbm.train(params, train_set = train_set, num_boost_round=300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_gb = gb.predict(X_tr[features2])\nrmse = np.sqrt(mean_squared_error(y_train_gb, y_tr))\nprint(\"train_rmse = \", rmse)\n\ny_valid_gb = gb.predict(X_va[features2])\nrmse = np.sqrt(mean_squared_error(y_valid_gb, y_va))\nprint(\"valid_rmse = \", rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\n# features = ['haverdist', 'abs_lat_diff', 'abs_lon_diff', 'passenger_count','pickup_Elapsed','manhattan','pickup_longitude','pickup_latitude', 'dropoff_longitude','dropoff_latitude']\n\n\ndtrain_xg = xgb.DMatrix(X_tr[features2], label=y_tr)\ndval_xg = xgb.DMatrix(X_va[features2])\n\nparams = {'max_depth':7,\n          'eta':1,\n          'silent':1,\n          'objective':'reg:linear',\n          'eval_metric':'rmse',\n          'learning_rate':0.05\n         }\nnum_rounds = 50\nxb = xgb.train(params, dtrain_xg, num_rounds)\n\n\n# y_train_xgb = xb.predict(dtrain)\n# rmse = np.sqrt(mean_squared_error(y_train_xgb, y_tr))\n# rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_xg = xb.predict(dtrain_xg)\nrmse = np.sqrt(mean_squared_error(y_train_xg, y_tr))\nprint(\"train_rmse = \", rmse)\n\n\ny_valid_xg = xb.predict(dval_xg)\nrmse2 = np.sqrt(mean_squared_error(y_valid_xg, y_va))\nprint(\"valid_rmse = \", rmse2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.plot_importance(xb)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MODEL SUBMISSION ###","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict_y = model.predict(test_X)\n\n# rerunning the model include the testing dataset \n# xb = xgb.train(params, dtrain, num_rounds)\npredict_y = gb.predict(X[X[\"dataset\"] == \"test\"][features2])\n\n# predict_y = model_lin.predict(test_X)\n# predict_y = np.expm1(predict_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict_y = predict_y[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create submission file\nsubmission = pd.DataFrame({'key': X[X[\"dataset\"] == \"test\"][\"key\"],\n    'fare_amount':predict_y.round(2)\n})\n\n\nsubmission.to_csv('taxi_fare_submission.csv',index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Special Thanks to \n\nhttps://www.kaggle.com/danpavlov/ny-taxi-fare-comprehensive-and-simple-analysis\n\n\nhttps://www.kaggle.com/willkoehrsen/a-walkthrough-and-a-challenge","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}